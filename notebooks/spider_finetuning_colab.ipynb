{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SQL Micro-Brain Fine-tuning on Spider Dataset\n",
        "\n",
        "This notebook demonstrates how to fine-tune SQL Micro-Brain on the Spider SQL dataset using Google Colab's free T4 GPU.\n",
        "\n",
        "## Prerequisites\n",
        "- Google Colab account\n",
        "- At least 16GB GPU RAM (T4 is sufficient)\n",
        "- Good internet connection for dataset download\n",
        "\n",
        "## What you'll learn\n",
        "- Setting up a Python environment for LLM fine-tuning\n",
        "- Processing the Spider dataset for instruction tuning\n",
        "- Fine-tuning a 1.5B parameter model with LoRA\n",
        "- Running inference on SQL generation tasks\n",
        "\n",
        "## Time estimates\n",
        "- Setup: ~10 minutes\n",
        "- Dataset download and preprocessing: ~15 minutes\n",
        "- Model training: ~2-4 hours\n",
        "- Inference demo: ~5 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "Install required packages and setup the environment. This includes:\n",
        "- Python packages for transformers, PEFT, datasets\n",
        "- Git for cloning the repository\n",
        "- Dependencies for GPU training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers accelerate peft datasets\n",
        "!pip install huggingface_hub\n",
        "\n",
        "# Verify GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory // (1024**3)}GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone SQL Micro-Brain Repository\n",
        "\n",
        "Clone the repository and navigate to the project directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/neilecm/SQL_MicroBrain.git\n",
        "%cd SQL_MicroBrain\n",
        "\n",
        "# List files to verify clone\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download Spider Dataset\n",
        "\n",
        "Download the Spider dataset which contains SQL queries paired with natural language questions. The dataset will be automatically extracted to `data/raw/spider/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Spider dataset\n",
        "!wget -P data/raw/ https://drive.google.com/uc?id=1nOd9T2S8AEq7Qm5WkHh_kwOXrHjdh3iS --no-check-certificate\n",
        "\n",
        "# Extract the dataset (assuming it's a zip file)\n",
        "!unzip -q data/raw/spider.zip -d data/raw/\n",
        "\n",
        "# Verify extraction\n",
        "!ls -la data/raw/spider/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build Training Data\n",
        "\n",
        "Process the Spider dataset into JSONL format suitable for instruction tuning. This involves:\n",
        "- Loading Spider JSON files\n",
        "- Integrating database schemas\n",
        "- Formatting into chat conversations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the training data preparation script\n",
        "!python training/scripts/build_spider_training_data.py\n",
        "\n",
        "# Check that the training data was created\n",
        "!ls -la data/processed/train_spider_sqlmb.jsonl\n",
        "!head -5 data/processed/train_spider_sqlmb.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n",
        "\n",
        "Fine-tune the Qwen2.5-Coder-1.5B-Instruct model using LoRA. This process:\n",
        "- Uses memory-efficient LoRA for parameter reduction\n",
        "- Optimizes hyperparameters for T4 GPU\n",
        "- Takes approximately 2-4 hours depending on dataset size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training (this will take several hours)\n",
        "!python training/scripts/train_lora_spider.py\n",
        "\n",
        "# Check that the model was saved\n",
        "!ls -la models/sql-micro-brain-spider-lora/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference Demo\n",
        "\n",
        "Test the fine-tuned model on SQL generation tasks. We'll create a simple inference script and demonstrate it with example queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create inference demo script\n",
        "%%writefile inference_demo.py\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "def setup_model():\n",
        "    \"\"\"Load the fine-tuned model\"\"\"\n",
        "    ROOT = Path(__file__).resolve().parents[0]\n",
        "    model_path = ROOT / \"models\" / \"sql-micro-brain-spider-lora\"\n",
        "    \n",
        "    # Load base model\n",
        "    base_model = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    \n",
        "    # Load LoRA adapter\n",
        "    model = PeftModel.from_pretrained(model, str(model_path))\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_sql(model, tokenizer, question, schema_sql, db_name=\"\"):\n",
        "    \"\"\"Generate SQL from natural language task and schema\"\"\"\n",
        "    system_prompt = (\n",
        "        \"You are SQL Micro-Brain, an expert PostgreSQL assistant. \"\n",
        "        \"Given a natural language task and a database schema, you output a JSON object \"\n",
        "        \"with actions, migrations, rls_policies, indexes, queries, error_explanations, \"\n",
        "        \"explanations, and safe_to_execute. You MUST produce valid PostgreSQL SQL.\"\n",
        "    )\n",
        "    \n",
        "    user_content = (\n",
        "        f\"Task: {question}\\n\\n\"\n",
        "        f\"Database: {db_name}\\n\\n\"\n",
        "        f\"Schema:\\n{schema_sql}\\n\"\n",
        "    )\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_content}\n",
        "    ]\n",
        "    \n",
        "    # Format conversation\n",
        "    input_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.1,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode response\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    \n",
        "    return generated_text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Loading model...\")\n",
        "    model, tokenizer = setup_model()\n",
        "    \n",
        "    # Example schema (simplified)\n",
        "    sample_schema = \"\"\"\n",
        "    CREATE TABLE student (\n",
        "        id INTEGER PRIMARY KEY,\n",
        "        name TEXT,\n",
        "        major TEXT\n",
        "    );\n",
        "    \n",
        "    CREATE TABLE course (\n",
        "        id INTEGER PRIMARY KEY,\n",
        "        name TEXT,\n",
        "        credits INTEGER\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    # Test questions\n",
        "    test_questions = [\n",
        "        \"What are the names of all students majoring in Computer Science?\",\n",
        "        \"How many credits does the database course have?\",\n",
        "        \"List all courses with more than 3 credits.\"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\n--- Inference Demo ---\\n\")\n",
        "    for question in test_questions:\n",
        "        print(f\"Question: {question}\")\n",
        "        response = generate_sql(model, tokenizer, question, sample_schema, \"university\")\n",
        "        print(f\"Response: {response}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the inference demo\n",
        "!python inference_demo.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Export and Save Results\n",
        "\n",
        "Zip the fine-tuned model and download for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zip the model directory\n",
        "!zip -r sql-micro-brain-spider-lora.zip models/sql-micro-brain-spider-lora/\n",
        "\n",
        "# Download the model (in Colab, this will trigger a download)\n",
        "from google.colab import files\n",
        "files.download('sql-micro-brain-spider-lora.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Cleanup and Tips\n",
        "\n",
        "### Memory Management\n",
        "- If you run out of memory during training, reduce `per_device_train_batch_size` to 1\n",
        "- Enable gradient checkpointing (already included) saves memory at cost of speed\n",
        "- Use mixed precision training (bf16 already enabled)\n",
        "\n",
        "### Troubleshooting\n",
        "- If dataset download fails, try alternative download methods\n",
        "- If training fails with CUDA errors, restart runtime and try again\n",
        "- Monitor GPU usage with `!nvidia-smi` in a separate cell\n",
        "\n",
        "### Next Steps\n",
        "- Evaluate model performance on test set using Spider evaluation script\n",
        "- Try different LoRA configurations or hyperparameters\n",
        "- Deploy the model for production use\n",
        "\n",
        "### Cost Considerations\n",
        "- Google Colab Pro: ~$10/month for faster GPUs and longer runtimes\n",
        "- Training time can be reduced with A100 GPUs (Colab Pro+)\n",
        "- Consider using cloud instances for production training"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
